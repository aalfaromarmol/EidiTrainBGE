# -*- coding: utf-8 -*-
"""Copy of AAM  Fine-tuning bge [Train]Maria

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KH0S3d6vq24zJ6HKWkhK7-K0MfBh0FSz
"""

#cp BGE TRAIN

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

DATA_PATH= eedi_mining_misconceptions_in_mathematics_path = kagglehub.competition_download('eedi-mining-misconceptions-in-mathematics')

print('Data source import complete.')

"""# Overview

- [Infer Notebook](https://www.kaggle.com/code/sinchir0/fine-tuning-bge-infer/notebook)

- make 25 retrieval data by `bge-large-en-v1.5`
- Fine-tuning `bge-large-en-v1.5` by retrieval data
  - `anchor`: `ConstructName` + `SubjectName` + `QuestionText` + `Answer[A-D]Text`
  - `positive`: Correct MisconceptionName
  - `negative`: Wrong MisconceptionName

ref: https://sbert.net/docs/sentence_transformer/training_overview.html#trainer

# Setting
"""

EXP_NAME = "fine-tuning-bge"

MODEL_NAME = "BAAI/bge-large-en-v1.5"
COMPETITION_NAME = "eedi-mining-misconceptions-in-mathematics"
OUTPUT_PATH = "."
MODEL_OUTPUT_PATH = f"{OUTPUT_PATH}/trained_model"

RETRIEVE_NUM = 25

EPOCH = 2
LR = 2e-05
BS = 8
GRAD_ACC_STEP = 128 // BS

TRAINING = True
DEBUG = False
WANDB = True

"""# Install"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qq polars==1.7.1
# %pip install -qq datasets==3.0.0
# %pip install -qq sentence_transformers==3.1.0

"""# Import"""

import os
import numpy as np

from datasets import load_dataset, Dataset

import wandb
import polars as pl

from sklearn.metrics.pairwise import cosine_similarity

from sentence_transformers.losses import MultipleNegativesRankingLoss
from sentence_transformers import (
    SentenceTransformer,
    SentenceTransformerTrainer,
    SentenceTransformerTrainingArguments,
)
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

import datasets
import sentence_transformers

assert pl.__version__ == "1.7.1"
assert datasets.__version__ == "3.0.0"
assert sentence_transformers.__version__ == "3.1.0"

NUM_PROC = os.cpu_count()

"""# WANDB"""

!pip install wandb

!wandb login d9f42311d58c8e3f2749d29f75c4d82cbdbd3b94

import os
import wandb

import wandb
import random

# start a new wandb run to track this script
wandb.init(
    # set the wandb project where this run will be logged
    project="Defining-Misconceptions-(BGE)",

    # track hyperparameters and run metadata
    config={
    "learning_rate": 0.02,
    "architecture": "CNN",
    "dataset": "CIFAR-100",
    "epochs": 10,
    }
)

# simulate training
epochs = 10
offset = random.random() / 5
for epoch in range(2, epochs):
    acc = 1 - 2 ** -epoch - random.random() / epoch - offset
    loss = 2 ** -epoch + random.random() / epoch + offset

    # log metrics to wandb
    wandb.log({"acc": acc, "loss": loss})

# [optional] finish the wandb run, necessary in notebooks
wandb.finish()

"""# Data Load"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Specify the folder containing the documents

train_csv = '/content/drive/MyDrive/6156MLProject/csv/train.csv'
misconception_mapping_csv = '/content/drive/MyDrive/6156MLProject/csv/misconception_mapping.csv'
test_csv = '/content/drive/MyDrive/6156MLProject/csv/test.csv'

!pip install kaggle

!kaggle competitions download -c eedi-mining-eedi-mining-misconceptions-in-mathematics

train = pl.read_csv(f"{DATA_PATH}/train.csv")
misconception_mapping = pl.read_csv(f"{DATA_PATH}/misconception_mapping.csv")

test = pl.read_csv(f"{DATA_PATH}/test.csv")
misconception_mapping = pl.read_csv(f"{DATA_PATH}/misconception_mapping.csv")

common_col = [
    "QuestionId",
    "ConstructName",
    "SubjectName",
    "QuestionText",
    "CorrectAnswer",
]

train_long = (
    train
  .select(
        pl.col(common_col + [f"Answer{alpha}Text" for alpha in ["A", "B", "C", "D"]])
    )
    .unpivot(
        index=common_col,
        variable_name="AnswerType",
        value_name="AnswerText",
    )
    .with_columns(
        pl.concat_str(
            [
                pl.col("ConstructName"),
                pl.col("SubjectName"),
                pl.col("QuestionText"),
                pl.col("AnswerText"),
            ],
            separator=" ",
        ).alias("AllText"),
        pl.col("AnswerType").str.extract(r"Answer([A-D])Text$").alias("AnswerAlphabet"),
    )
    .with_columns(
        pl.concat_str(
            [pl.col("QuestionId"), pl.col("AnswerAlphabet")], separator="_"
        ).alias("QuestionId_Answer"),
    )
    .sort("QuestionId_Answer")
)
train_long.head()

import polars as pl

# Define common columns
common_col = [
    "QuestionId",
    "ConstructName",
    "SubjectName",
    "QuestionText",
    "CorrectAnswer",
]

# Transform the DataFrame
train_long = (
    train
    .select(
        pl.col(common_col + [f"Answer{alpha}Text" for alpha in ["A", "B", "C", "D"]])
    )
    .melt(
        id_vars=common_col,
        value_vars=[f"Answer{alpha}Text" for alpha in ["A", "B", "C", "D"]],
        variable_name="AnswerType",
        value_name="AnswerText",
    )
    .with_columns(
        pl.concat_str(
            [
                pl.col("ConstructName"),
                pl.col("SubjectName"),
                pl.col("QuestionText"),
                pl.col("AnswerText"),
            ],
            separator=" ",
        ).alias("AllText"),
        pl.col("AnswerType").str.extract(r"Answer([A-D])Text$").alias("AnswerAlphabet"),
    )
    .with_columns(
        pl.concat_str(
            [pl.col("QuestionId"), pl.col("AnswerAlphabet")], separator="_"
        ).alias("QuestionId_Answer"),
    )
    .sort("QuestionId_Answer")
)

# Display the first few rows
train_long.head()

train_misconception_long = (
    train.select(
        pl.col(
            common_col + [f"Misconception{alpha}Id" for alpha in ["A", "B", "C", "D"]]
        )
    )
    .unpivot(
        index=common_col,
        variable_name="MisconceptionType",
        value_name="MisconceptionId",
    )
    .with_columns(
        pl.col("MisconceptionType")
        .str.extract(r"Misconception([A-D])Id$")
        .alias("AnswerAlphabet"),
    )
    .with_columns(
        pl.concat_str(
            [pl.col("QuestionId"), pl.col("AnswerAlphabet")], separator="_"
        ).alias("QuestionId_Answer"),
    )
    .sort("QuestionId_Answer")
    .select(pl.col(["QuestionId_Answer", "MisconceptionId"]))
    .with_columns(pl.col("MisconceptionId").cast(pl.Int64))
)

train_misconception_long.head()

# join MisconceptionId
train_long = train_long.join(train_misconception_long, on="QuestionId_Answer")
train_long.head()

"""# Make retrieval data"""

# Import required libraries
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm
import numpy as np
import torch
import gc

#hugging face token: HF_TOKEN
#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

MODEL_NAME = "all-MiniLM-L6-v2"
model = SentenceTransformer(MODEL_NAME)

train_long_vec = model.encode(
    train_long["AllText"].to_list(), normalize_embeddings=True
)
misconception_mapping_vec = model.encode(
    misconception_mapping["MisconceptionName"].to_list(), normalize_embeddings=True
)
print(train_long_vec.shape)
print(misconception_mapping_vec.shape)

train_cos_sim_arr = cosine_similarity(train_long_vec, misconception_mapping_vec)
train_sorted_indices = np.argsort(-train_cos_sim_arr, axis=1)

train_long = train_long.with_columns(
    pl.Series(train_sorted_indices[:, :RETRIEVE_NUM].tolist()).alias(
        "PredictMisconceptionId"
    )
)

train_retrieved = (
    train_long.filter(
        pl.col(
            "MisconceptionId"
        ).is_not_null()  # TODO: Consider ways to utilize data where MisconceptionId is NaN.
    )
    .explode("PredictMisconceptionId")
    .join(
        misconception_mapping,
        on="MisconceptionId",
    )
    .join(
        misconception_mapping.rename(lambda x: "Predict" + x),
        on="PredictMisconceptionId",
    )
)
train_retrieved.shape

"""# Fine-Tune bge"""

train = (
    Dataset.from_polars(train_retrieved)
    .filter(  # To create an anchor, positive, and negative structure, delete rows where the positive and negative are identical.
        lambda example: example["MisconceptionId"] != example["PredictMisconceptionId"],
        num_proc=NUM_PROC,
    )
)

train

if DEBUG:
    train = train.select(range(1000))
    EPOCH = 1

from sentence_transformers import SentenceTransformer, losses, LoggingHandler
import logging
import os
import torch
from typing import List, Dict, Optional
import wandb
from pathlib import Path

class SentenceTransformerTrainer:
    """
    A trainer class for fine-tuning Sentence Transformer models.
    """
    def __init__(self, config: Dict):
        """
        Initialize the trainer with configuration parameters.

        Args:
            config (Dict): Configuration dictionary containing model parameters
        """
        self.config = self._validate_config(config)
        self._setup_logging()
        self._setup_paths()
        self._setup_device()
        self.model = None
        self.loss_fn = None

    @staticmethod
    def _validate_config(config: Dict) -> Dict:
        """Validate and set default configuration values."""
        defaults = {
            "model_name": "all-MiniLM-L6-v2",
            "output_path": "./output",
            "model_output_path": "./output/final_model",
            "epochs": 3,
            "batch_size": 32,
            "gradient_accumulation_steps": 1,
            "learning_rate": 2e-5,
            "warmup_ratio": 0.1,
            "weight_decay": 0.01,
            "evaluation_steps": 1000,
            "save_best_model": True,
            "report_to": ["wandb"],
            "experiment_name": "sentence_transformer_training"
        }

        # Update defaults with provided config
        defaults.update(config)
        return defaults

    def _setup_logging(self):
        """Configure logging settings."""
        logging.basicConfig(
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            level=logging.INFO,
            handlers=[LoggingHandler()]
        )
        self.logger = logging.getLogger(__name__)

    def _setup_paths(self):
        """Create necessary directories."""
        Path(self.config['output_path']).mkdir(parents=True, exist_ok=True)
        Path(self.config['model_output_path']).mkdir(parents=True, exist_ok=True)

    def _setup_device(self):
        """Configure device settings."""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger.info(f"Using device: {self.device}")

    def initialize_model(self):
        """Initialize the model and loss function."""
        try:
            self.model = SentenceTransformer(self.config['model_name'])
            self.model.to(self.device)
            self.loss_fn = losses.MultipleNegativesRankingLoss(self.model)
            self.logger.info(f"Model initialized: {self.config['model_name']}")
        except Exception as e:
            self.logger.error(f"Error initializing model: {str(e)}")
            raise

    def _setup_wandb(self):
        """Initialize Weights & Biases logging if enabled."""
        if "wandb" in self.config['report_to']:
            wandb.init(
                project=self.config['experiment_name'],
                config=self.config,
                name=f"{self.config['model_name']}_training"
            )

    def get_training_args(self) -> Dict:
        """
        Get training arguments for the model.

        Returns:
            Dict: Training arguments
        """
        return {
            "output_dir": self.config['output_path'],
            "num_epochs": self.config['epochs'],
            "train_batch_size": self.config['batch_size'],
            "learning_rate": self.config['learning_rate'],
            "warmup_ratio": self.config['warmup_ratio'],
            "weight_decay": self.config['weight_decay'],
            "evaluation_steps": self.config['evaluation_steps'],
            "save_best_model": self.config['save_best_model'],
            "gradient_accumulation_steps": self.config['gradient_accumulation_steps'],
            "device": self.device
        }

    def train(self, train_dataset, eval_dataset=None):
        """
        Train the model.

        Args:
            train_dataset: Dataset for training
            eval_dataset: Optional dataset for evaluation
        """
        try:
            if not self.model:
                self.initialize_model()

            if "wandb" in self.config['report_to']:
                self._setup_wandb()

            train_args = self.get_training_args()

            self.logger.info("Starting training...")
            self.model.fit(
                train_objectives=[(train_dataset, self.loss_fn)],
                evaluator=eval_dataset,
                **train_args
            )

            # Save the final model
            self.model.save(self.config['model_output_path'])
            self.logger.info(f"Model saved to {self.config['model_output_path']}")

        except Exception as e:
            self.logger.error(f"Error during training: {str(e)}")
            raise
        finally:
            if "wandb" in self.config['report_to']:
                wandb.finish()

    def evaluate(self, eval_dataset):
        """
        Evaluate the model on a test dataset.

        Args:
            eval_dataset: Dataset for evaluation
        """
        if not self.model:
            raise ValueError("Model not initialized. Call initialize_model() first.")

        try:
            self.logger.info("Starting evaluation...")
            results = self.model.evaluate(eval_dataset)
            self.logger.info(f"Evaluation results: {results}")
            return results
        except Exception as e:
            self.logger.error(f"Error during evaluation: {str(e)}")
            raise

# Example usage
if __name__ == "__main__":
    config = {
        "model_name": "all-MiniLM-L6-v2",
        "output_path": "./output",
        "model_output_path": "./output/final_model",
        "epochs": 3,
        "batch_size": 32,
        "learning_rate": 2e-5,
        "report_to": ["wandb"]
    }

    trainer = SentenceTransformerTrainer(config)

!pip install sentence-transformers torch transformers datasets

from sentence_transformers import SentenceTransformer, losses, LoggingHandler
import logging
import os
import torch
from typing import List, Dict, Optional
import wandb
from pathlib import Path

class SentenceTransformerTrainer:
    """
    A trainer class for fine-tuning Sentence Transformer models.
    """
    def __init__(self, config: Dict):
        """
        Initialize the trainer with configuration parameters.

        Args:
            config (Dict): Configuration dictionary containing model parameters
        """
        self.config = self._validate_config(config)
        self._setup_logging()
        self._setup_paths()
        self._setup_device()
        self.model = None
        self.loss_fn = None

    @staticmethod
    def _validate_config(config: Dict) -> Dict:
        """Validate and set default configuration values."""
        defaults = {
            "model_name": "all-MiniLM-L6-v2",
            "output_path": "./output",
            "model_output_path": "./output/final_model",
            "epochs": 3,
            "batch_size": 32,
            "gradient_accumulation_steps": 1,
            "learning_rate": 2e-5,
            "warmup_ratio": 0.1,
            "weight_decay": 0.01,
            "evaluation_steps": 1000,
            "save_best_model": True,
            "report_to": ["wandb"],
            "experiment_name": "sentence_transformer_training"
        }

        # Update defaults with provided config
        defaults.update(config)
        return defaults

    def _setup_logging(self):
        """Configure logging settings."""
        logging.basicConfig(
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            level=logging.INFO,
            handlers=[LoggingHandler()]
        )
        self.logger = logging.getLogger(__name__)

    def _setup_paths(self):
        """Create necessary directories."""
        Path(self.config['output_path']).mkdir(parents=True, exist_ok=True)
        Path(self.config['model_output_path']).mkdir(parents=True, exist_ok=True)

    def _setup_device(self):
        """Configure device settings."""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger.info(f"Using device: {self.device}")

    def initialize_model(self):
        """Initialize the model and loss function."""
        try:
            self.model = SentenceTransformer(self.config['model_name'])
            self.model.to(self.device)
            self.loss_fn = losses.MultipleNegativesRankingLoss(self.model)
            self.logger.info(f"Model initialized: {self.config['model_name']}")
        except Exception as e:
            self.logger.error(f"Error initializing model: {str(e)}")
            raise

    def _setup_wandb(self):
        """Initialize Weights & Biases logging if enabled."""
        if "wandb" in self.config['report_to']:
            wandb.init(
                project=self.config['experiment_name'],
                config=self.config,
                name=f"{self.config['model_name']}_training"
            )

    def get_training_args(self) -> Dict:
        """
        Get training arguments for the model.

        Returns:
            Dict: Training arguments
        """
        return {
            "output_dir": self.config['output_path'],
            "num_epochs": self.config['epochs'],
            "train_batch_size": self.config['batch_size'],
            "learning_rate": self.config['learning_rate'],
            "warmup_ratio": self.config['warmup_ratio'],
            "weight_decay": self.config['weight_decay'],
            "evaluation_steps": self.config['evaluation_steps'],
            "save_best_model": self.config['save_best_model'],
            "gradient_accumulation_steps": self.config['gradient_accumulation_steps'],
            "device": self.device
        }

    def train(self, train_dataset, eval_dataset=None):
        """
        Train the model.

        Args:
            train_dataset: Dataset for training
            eval_dataset: Optional dataset for evaluation
        """
        try:
            if not self.model:
                self.initialize_model()

            if "wandb" in self.config['report_to']:
                self._setup_wandb()

            train_args = self.get_training_args()

            self.logger.info("Starting training...")
            self.model.fit(
                train_objectives=[(train_dataset, self.loss_fn)],
                evaluator=eval_dataset,
                **train_args
            )

            # Save the final model
            self.model.save(self.config['model_output_path'])
            self.logger.info(f"Model saved to {self.config['model_output_path']}")

        except Exception as e:
            self.logger.error(f"Error during training: {str(e)}")
            raise
        finally:
            if "wandb" in self.config['report_to']:
                wandb.finish()

    def evaluate(self, eval_dataset):
        """
        Evaluate the model on a test dataset.

        Args:
            eval_dataset: Dataset for evaluation
        """
        if not self.model:
            raise ValueError("Model not initialized. Call initialize_model() first.")

        try:
            self.logger.info("Starting evaluation...")
            results = self.model.evaluate(eval_dataset)
            self.logger.info(f"Evaluation results: {results}")
            return results
        except Exception as e:
            self.logger.error(f"Error during evaluation: {str(e)}")
            raise

# Example usage
if __name__ == "__main__":
    config = {
        "model_name": "all-MiniLM-L6-v2",
        "output_path": "./output",
        "model_output_path": "./output/final_model",
        "epochs": 3,
        "batch_size": 32,
        "learning_rate": 2e-5,
        "report_to": ["wandb"]
    }

    trainer = SentenceTransformerTrainer(config)

from sentence_transformers import SentenceTransformer, losses, LoggingHandler
import logging
import os
import torch
from typing import List, Dict, Optional
import wandb
from pathlib import Path

class SentenceTransformerTrainer:
    """
    A trainer class for fine-tuning Sentence Transformer models.
    """
    def __init__(self, config: Dict):
        """
        Initialize the trainer with configuration parameters.

        Args:
            config (Dict): Configuration dictionary containing model parameters
        """
        self.config = self._validate_config(config)
        self._setup_logging()
        self._setup_paths()
        self._setup_device()
        self.model = None
        self.loss_fn = None

    @staticmethod
    def _validate_config(config: Dict) -> Dict:
        """Validate and set default configuration values."""
        defaults = {
            "model_name": "all-MiniLM-L6-v2",
            "output_path": "./output",
            "model_output_path": "./output/final_model",
            "epochs": 3,
            "batch_size": 32,
            "gradient_accumulation_steps": 1,
            "learning_rate": 2e-5,
            "warmup_ratio": 0.1,
            "weight_decay": 0.01,
            "evaluation_steps": 1000,
            "save_best_model": True,
            "report_to": ["wandb"],
            "experiment_name": "sentence_transformer_training"
        }

        # Update defaults with provided config
        defaults.update(config)
        return defaults

    def _setup_logging(self):
        """Configure logging settings."""
        logging.basicConfig(
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            level=logging.INFO,
            handlers=[LoggingHandler()]
        )
        self.logger = logging.getLogger(__name__)

    def _setup_paths(self):
        """Create necessary directories."""
        Path(self.config['output_path']).mkdir(parents=True, exist_ok=True)
        Path(self.config['model_output_path']).mkdir(parents=True, exist_ok=True)

    def _setup_device(self):
        """Configure device settings."""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger.info(f"Using device: {self.device}")

    def initialize_model(self):
        """Initialize the model and loss function."""
        try:
            self.model = SentenceTransformer(self.config['model_name'])
            self.model.to(self.device)
            self.loss_fn = losses.MultipleNegativesRankingLoss(self.model)
            self.logger.info(f"Model initialized: {self.config['model_name']}")
        except Exception as e:
            self.logger.error(f"Error initializing model: {str(e)}")
            raise

    def _setup_wandb(self):
        """Initialize Weights & Biases logging if enabled."""
        if "wandb" in self.config['report_to']:
            wandb.init(
                project=self.config['experiment_name'],
                config=self.config,
                name=f"{self.config['model_name']}_training"
            )

    def get_training_args(self) -> Dict:
        """
        Get training arguments for the model.

        Returns:
            Dict: Training arguments
        """
        return {
            "output_dir": self.config['output_path'],
            "num_epochs": self.config['epochs'],
            "train_batch_size": self.config['batch_size'],
            "learning_rate": self.config['learning_rate'],
            "warmup_ratio": self.config['warmup_ratio'],
            "weight_decay": self.config['weight_decay'],
            "evaluation_steps": self.config['evaluation_steps'],
            "save_best_model": self.config['save_best_model'],
            "gradient_accumulation_steps": self.config['gradient_accumulation_steps'],
            "device": self.device
        }

    def train(self, train_dataset, eval_dataset=None):
        """
        Train the model.

        Args:
            train_dataset: Dataset for training
            eval_dataset: Optional dataset for evaluation
        """
        try:
            if not self.model:
                self.initialize_model()

            if "wandb" in self.config['report_to']:
                self._setup_wandb()

            train_args = self.get_training_args()

            self.logger.info("Starting training...")
            self.model.fit(
                train_objectives=[(train_dataset, self.loss_fn)],
                evaluator=eval_dataset,
                **train_args
            )

            # Save the final model
            self.model.save(self.config['model_output_path'])
            self.logger.info(f"Model saved to {self.config['model_output_path']}")

        except Exception as e:
            self.logger.error(f"Error during training: {str(e)}")
            raise
        finally:
            if "wandb" in self.config['report_to']:
                wandb.finish()

    def evaluate(self, eval_dataset):
        """
        Evaluate the model on a test dataset.

        Args:
            eval_dataset: Dataset for evaluation
        """
        if not self.model:
            raise ValueError("Model not initialized. Call initialize_model() first.")

        try:
            self.logger.info("Starting evaluation...")
            results = self.model.evaluate(eval_dataset)
            self.logger.info(f"Evaluation results: {results}")
            return results
        except Exception as e:
            self.logger.error(f"Error during evaluation: {str(e)}")
            raise

# Example usage
if __name__ == "__main__":
    config = {
        "model_name": "all-MiniLM-L6-v2",
        "output_path": "./output",
        "model_output_path": "./output/final_model",
        "epochs": 3,
        "batch_size": 32,
        "learning_rate": 2e-5,
        "report_to": ["wandb"]
    }

    trainer = SentenceTransformerTrainer(config)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score

train_data = pd.read_csv('train.csv')
misconception_mapping_data = pd.read_csv('misconception_mapping.csv')
test_data = pd.read_csv('test.csv')

train_data

misconception_mapping_data

test_data

label_encoder = LabelEncoder()

def preprocess_data(data):
  label_encoder = LabelEncoder()
  data['CorrectAnswer'] = label_encoder.fit_transform(data['CorrectAnswer'])
  return data, label_encoder

train_data, label_encoder = preprocess_data(train_data)
test_data, _ = preprocess_data(test_data)

X_train = train_data.drop('CorrectAnswer', axis=1)
y_train = train_data['CorrectAnswer']
X_test = test_data.drop('CorrectAnswer', axis=1)
y_test = test_data['CorrectAnswer']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R_Squared Score:", r2)